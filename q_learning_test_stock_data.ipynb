{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\r\n",
                "import torch.nn as nn\r\n",
                "import torch.optim as optim\r\n",
                "#import torch.nn.functional as F\r\n",
                "import math\r\n",
                "import numpy as np\r\n",
                "import random\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from collections import namedtuple, deque\r\n",
                "import datetime\r\n",
                "\r\n",
                "from sklearn import preprocessing\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "import yfinance as yf\r\n",
                "\r\n",
                "from torch.utils.tensorboard import SummaryWriter\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "log_dir = \"stock_data_runs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
                "writer = SummaryWriter(log_dir)\r\n",
                "\r\n",
                "class DQN(nn.Module):\r\n",
                "\r\n",
                "    def __init__(self, inputs, outputs):\r\n",
                "        super(DQN, self).__init__()\r\n",
                "        self.linear_relu_stack_val = nn.Sequential(\r\n",
                "            nn.Linear(inputs, 50),\r\n",
                "            nn.Dropout(0.1),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(50, 50),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(50, 1),\r\n",
                "        )\r\n",
                "\r\n",
                "        self.linear_relu_stack_adv = nn.Sequential(\r\n",
                "            nn.Linear(inputs, 50),\r\n",
                "            nn.Dropout(0.1),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(50, 50),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(50, outputs),\r\n",
                "        )\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        x.to(device)\r\n",
                "\r\n",
                "        val = self.linear_relu_stack_val(x)\r\n",
                "        adv = self.linear_relu_stack_adv(x)\r\n",
                "\r\n",
                "        return val + adv - adv.mean()\r\n",
                "\r\n",
                "class DQN_mid(nn.Module):\r\n",
                "\r\n",
                "    def __init__(self, inputs, outputs):\r\n",
                "        super(DQN, self).__init__()\r\n",
                "        self.linear_relu_stack = nn.Sequential(\r\n",
                "            nn.Linear(inputs, 2000),\r\n",
                "            nn.Dropout(0.1),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 1000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(1000, 500),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(500, 200),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(200, outputs),\r\n",
                "        )\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        x.to(device)\r\n",
                "\r\n",
                "        x = self.linear_relu_stack(x)\r\n",
                "        return x\r\n",
                "        #return self.head(x.view(x.size(0), -1))\r\n",
                "\r\n",
                "class DQN_big(nn.Module):\r\n",
                "\r\n",
                "    def __init__(self, inputs, outputs):\r\n",
                "        super(DQN, self).__init__()\r\n",
                "        self.linear_relu_stack = nn.Sequential(\r\n",
                "            nn.Linear(inputs, 2000),\r\n",
                "            nn.Dropout(0.1),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 2000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(2000, 1000),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(1000, 500),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(500, 200),\r\n",
                "            nn.Dropout(0.5),\r\n",
                "            nn.ReLU(),\r\n",
                "            nn.Linear(200, outputs),\r\n",
                "        )\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        x.to(device)\r\n",
                "\r\n",
                "        x = self.linear_relu_stack(x)\r\n",
                "        return x\r\n",
                "        #return self.head(x.view(x.size(0), -1))\r\n",
                "\r\n",
                "torch.manual_seed(0)\r\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
                "\r\n",
                "\r\n",
                "loss_timeline = []"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "Transition = namedtuple('Transition',\r\n",
                "                        ('state', 'action', 'next_state', 'reward'))\r\n",
                "\r\n",
                "\r\n",
                "class ReplayMemory(object):\r\n",
                "\r\n",
                "    def __init__(self, capacity):\r\n",
                "        self.memory = deque([],maxlen=capacity)\r\n",
                "\r\n",
                "    def push(self, *args):\r\n",
                "        \"\"\"Save a transition\"\"\"\r\n",
                "        self.memory.append(Transition(*args))\r\n",
                "\r\n",
                "    def sample(self, batch_size):\r\n",
                "        return random.sample(self.memory, batch_size)\r\n",
                "\r\n",
                "    def __len__(self):\r\n",
                "        return len(self.memory)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "\r\n",
                "\r\n",
                "time_intervall = 100\r\n",
                "\r\n",
                "input_size = time_intervall * 3 + 1\r\n",
                "n_actions = 3\r\n",
                "\r\n",
                "policy_net = DQN(input_size, n_actions).to(device)\r\n",
                "target_net = DQN(input_size, n_actions).to(device)\r\n",
                "target_net.load_state_dict(policy_net.state_dict())\r\n",
                "target_net.eval()\r\n",
                "\r\n",
                "optimizer = optim.RMSprop(policy_net.parameters())\r\n",
                "memory = ReplayMemory(1000000)\r\n",
                "\r\n",
                "steps_done = 0\r\n",
                "\r\n",
                "def select_action(state):\r\n",
                "    global steps_done\r\n",
                "    sample = random.random()\r\n",
                "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\r\n",
                "        math.exp(-1. * steps_done / EPS_DECAY)\r\n",
                "    if steps_done % 100 == 0:\r\n",
                "                writer.add_scalar('eps_threshold', eps_threshold, steps_done)\r\n",
                "    steps_done += 1\r\n",
                "    if sample > eps_threshold:\r\n",
                "        with torch.no_grad():\r\n",
                "            # t.max(1) will return largest column value of each row.\r\n",
                "            # second column on max result is index of where max element was\r\n",
                "            # found, so we pick action with the larger expected reward.\r\n",
                "            #output = torch.argmax(policy_net(state))\r\n",
                "            return policy_net(state).max(1)[1].view(1, 1)\r\n",
                "    else:\r\n",
                "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def optimize_model():\r\n",
                "    if len(memory) < BATCH_SIZE:\r\n",
                "        return\r\n",
                "    transitions = memory.sample(BATCH_SIZE)\r\n",
                "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\r\n",
                "    # detailed explanation). This converts batch-array of Transitions\r\n",
                "    # to Transition of batch-arrays.\r\n",
                "    batch = Transition(*zip(*transitions))\r\n",
                "\r\n",
                "    # Compute a mask of non-final states and concatenate the batch elements\r\n",
                "    # (a final state would've been the one after which simulation ended)\r\n",
                "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\r\n",
                "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\r\n",
                "    state_batch = torch.cat(batch.state, dim=0)\r\n",
                "    action_batch = torch.cat(batch.action)\r\n",
                "    reward_batch = torch.cat(batch.reward)\r\n",
                "\r\n",
                "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\r\n",
                "    # columns of actions taken. These are the actions which would've been taken\r\n",
                "    # for each batch state according to policy_net\r\n",
                "    state_action_values = policy_net(state_batch).gather(1, action_batch)\r\n",
                "\r\n",
                "    # Compute V(s_{t+1}) for all next states.\r\n",
                "    # Expected values of actions for non_final_next_states are computed based\r\n",
                "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\r\n",
                "    # This is merged based on the mask, such that we'll have either the expected\r\n",
                "    # state value or 0 in case the state was final.\r\n",
                "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\r\n",
                "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\r\n",
                "    # Compute the expected Q values\r\n",
                "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\r\n",
                "\r\n",
                "    # Compute Huber loss\r\n",
                "    criterion = nn.SmoothL1Loss()\r\n",
                "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\r\n",
                "\r\n",
                "    # Optimize the model\r\n",
                "    optimizer.zero_grad()\r\n",
                "    loss.backward()\r\n",
                "    for param in policy_net.parameters():\r\n",
                "        param.grad.data.clamp_(-1, 1)\r\n",
                "    optimizer.step()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "ORDER_COMMISSION = 1.01\r\n",
                "random.seed(a=1337)\r\n",
                "\r\n",
                "REWARD_SMOOTHING = 0.999\r\n",
                "ACTION_SMOOTHING = 0.9999\r\n",
                "\r\n",
                "avg_action_0 = 0.33\r\n",
                "avg_action_1 = 0.33\r\n",
                "avg_action_2 = 0.33\r\n",
                "\r\n",
                "steps_done_amount = 100\r\n",
                "forward_looking_timeframe = 10000\r\n",
                "\r\n",
                "reward_array = 0"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "stock_names = ['^GDAXI','MSFT','AAPL','AMZN','DIS']\r\n",
                "stock_datas = []\r\n",
                "for name in stock_names:\r\n",
                "    hist = yf.Ticker(name).history(period=\"max\")\r\n",
                "    stock_datas.append(hist.values)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "BATCH_SIZE = 64\r\n",
                "GAMMA = 0.999\r\n",
                "EPS_START = 0.99\r\n",
                "EPS_END = 0.001\r\n",
                "EPS_DECAY = 10000\r\n",
                "OPTIMIZE_MODEL = 1\r\n",
                "TARGET_UPDATE = 100 * OPTIMIZE_MODEL"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "num_episodes = 1\r\n",
                "num_test = 1\r\n",
                "time_step_size = 1\r\n",
                "\r\n",
                "for i_episode in range(num_episodes):\r\n",
                "\r\n",
                "    print(i_episode + 1)\r\n",
                "\r\n",
                "    for stock_data in stock_datas:\r\n",
                "\r\n",
                "        print('new stock')\r\n",
                "\r\n",
                "        MONEY = 1.0\r\n",
                "        AKTIEN = 0\r\n",
                "\r\n",
                "        time_step = time_intervall\r\n",
                "\r\n",
                "        done = False\r\n",
                "\r\n",
                "        buying_price = 0.0\r\n",
                "\r\n",
                "        next_data = [buying_price]\r\n",
                "\r\n",
                "        for i in range(time_intervall):\r\n",
                "                #next_data.append(stock_data[time_step - time_intervall + i][0])\r\n",
                "                next_data.append(stock_data[time_step - time_intervall + i][1])\r\n",
                "                next_data.append(stock_data[time_step - time_intervall + i][2])\r\n",
                "                next_data.append(stock_data[time_step - time_intervall + i][3])\r\n",
                "\r\n",
                "        while time_step < len(stock_data):\r\n",
                "\r\n",
                "            PREIS = stock_data[time_step][3]\r\n",
                "            PREIS_ALT = stock_data[time_step - 1][3]\r\n",
                "            MONEY_ALT = MONEY\r\n",
                "            AKTIEN_ALT = AKTIEN * PREIS_ALT\r\n",
                "\r\n",
                "            data = next_data\r\n",
                "\r\n",
                "            state = torch.tensor([data], device=device)\r\n",
                "            # Select and perform an action\r\n",
                "            action = select_action(state)\r\n",
                "\r\n",
                "            if action.item() == 0:\r\n",
                "                #kaufen\r\n",
                "                avg_action_0 = avg_action_0 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 1\r\n",
                "                avg_action_1 = avg_action_1 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                avg_action_2 = avg_action_2 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                if buying_price == 0:\r\n",
                "                    AKTIEN += (MONEY * 1) / (PREIS * ORDER_COMMISSION)\r\n",
                "                    MONEY -= MONEY * 1\r\n",
                "                    buying_price = PREIS * ORDER_COMMISSION\r\n",
                "                    reward = 1 - ORDER_COMMISSION\r\n",
                "                else:\r\n",
                "                    reward = -1.0\r\n",
                "\r\n",
                "            elif action.item() == 1:\r\n",
                "                #verkaufen\r\n",
                "                avg_action_0 = avg_action_0 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                avg_action_1 = avg_action_1 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 1\r\n",
                "                avg_action_2 = avg_action_2 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                if buying_price != 0:\r\n",
                "                    MONEY += AKTIEN * (PREIS / ORDER_COMMISSION)\r\n",
                "                    AKTIEN = 0\r\n",
                "                    reward = 100 * ((PREIS / ORDER_COMMISSION) - buying_price) / buying_price\r\n",
                "                    buying_price = 0.0\r\n",
                "                else:\r\n",
                "                    reward = -1.0\r\n",
                "\r\n",
                "            elif action.item() == 2:\r\n",
                "                avg_action_0 = avg_action_0 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                avg_action_1 = avg_action_1 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 0\r\n",
                "                avg_action_2 = avg_action_2 * ACTION_SMOOTHING + (1 - ACTION_SMOOTHING) * 1\r\n",
                "                reward = 0.0\r\n",
                "\r\n",
                "            time_step += time_step_size\r\n",
                "\r\n",
                "            if time_step >= len(stock_data):\r\n",
                "                MONEY += AKTIEN * PREIS\r\n",
                "                AKTIEN = 0\r\n",
                "                break\r\n",
                "\r\n",
                "            #reward = (MONEY + AKTIEN * PREIS) - (MONEY_ALT + AKTIEN_ALT)\r\n",
                "            \r\n",
                "            reward_array = reward_array * REWARD_SMOOTHING + (1 - REWARD_SMOOTHING) * reward\r\n",
                "\r\n",
                "            if steps_done % steps_done_amount == 0:\r\n",
                "                writer.add_scalar('avg reward', reward_array, steps_done)\r\n",
                "                writer.add_scalar('MONEY', MONEY + AKTIEN * PREIS, steps_done)\r\n",
                "                writer.add_scalar('avg action 0', avg_action_0, steps_done)\r\n",
                "                writer.add_scalar('avg action 1', avg_action_1, steps_done)\r\n",
                "                writer.add_scalar('avg action 2', avg_action_2, steps_done)\r\n",
                "\r\n",
                "            reward = torch.tensor([reward], device=device)\r\n",
                "\r\n",
                "            # Observe new state\r\n",
                "            if not done:\r\n",
                "                next_data = [buying_price]\r\n",
                "\r\n",
                "                for i in range(time_intervall):\r\n",
                "                        #next_data.append(stock_data[time_step - time_intervall + i][0])\r\n",
                "                        next_data.append(stock_data[time_step - time_intervall + i][1])\r\n",
                "                        next_data.append(stock_data[time_step - time_intervall + i][2])\r\n",
                "                        next_data.append(stock_data[time_step - time_intervall + i][3])\r\n",
                "\r\n",
                "                next_state = torch.tensor([next_data], device=device)\r\n",
                "            else:\r\n",
                "                next_state = None\r\n",
                "\r\n",
                "            # Store the transition in memory\r\n",
                "            memory.push(state, action, next_state, reward)\r\n",
                "\r\n",
                "            # Perform one step of the optimization (on the policy network)\r\n",
                "            if (steps_done) % OPTIMIZE_MODEL == 0:\r\n",
                "                optimize_model()\r\n",
                "\r\n",
                "            # Update the target network, copying all weights and biases in DQN\r\n",
                "            if (steps_done) % TARGET_UPDATE == 0:\r\n",
                "                target_net.load_state_dict(policy_net.state_dict())\r\n",
                "\r\n",
                "            if done:\r\n",
                "                break\r\n",
                "\r\n",
                "print('Complete')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "1\n",
                        "new stock\n",
                        "new stock\n",
                        "new stock\n",
                        "new stock\n",
                        "new stock\n",
                        "Complete\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "something"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.6 64-bit"
        },
        "interpreter": {
            "hash": "c8d051fc862597bd748cee664b4c849c53924153740f000520b0857a61b0c191"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}